import torch
import time
import json
import platform
import psutil
import numpy as np
from PIL import Image
from enum import Enum
from pathlib import Path
from transformers import AutoModelForImageTextToText, AutoProcessor, AutoTokenizer, BitsAndBytesConfig
from huggingface_hub import snapshot_download as hf_snapshot_download
import folder_paths
import gc

# å°è¯•å¯¼å…¥ ModelScopeï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨ HuggingFace
try:
    from modelscope.hub.snapshot_download import snapshot_download as ms_snapshot_download
    MODELSCOPE_AVAILABLE = True
except ImportError:
    ms_snapshot_download = None
    MODELSCOPE_AVAILABLE = False
    print("[Qwen3VL] âš ï¸ ModelScope æœªå®‰è£…ï¼ŒModelScope æ¨¡å‹å°†æ— æ³•ä¸‹è½½ã€‚è¯·è¿è¡Œ: pip install modelscope")

NODE_DIR = Path(__file__).parent
CONFIG_PATH = NODE_DIR / "config.json"
MODEL_CONFIGS = {}
SYSTEM_PROMPTS = {}

def load_model_configs():
    """åŠ è½½æ¨¡å‹é…ç½®æ–‡ä»¶"""
    global MODEL_CONFIGS, SYSTEM_PROMPTS
    try:
        with open(CONFIG_PATH, 'r', encoding='utf-8') as f:
            MODEL_CONFIGS = json.load(f)
            SYSTEM_PROMPTS = MODEL_CONFIGS.get("_system_prompts", {})
    except FileNotFoundError:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶æœªæ‰¾åˆ° {CONFIG_PATH}")
        MODEL_CONFIGS, SYSTEM_PROMPTS = {}, {}
    except json.JSONDecodeError:
        print(f"é”™è¯¯: é…ç½®æ–‡ä»¶è§£æå¤±è´¥")
        MODEL_CONFIGS, SYSTEM_PROMPTS = {}, {}

    # åŠ è½½ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹é…ç½®
    custom_path = NODE_DIR / "custom_models.json"
    if custom_path.exists():
        try:
            with open(custom_path, "r", encoding="utf-8") as f:
                custom_data = json.load(f) or {}
            
            user_models = custom_data.get("hf_models", {}) or custom_data.get("models", {})
            
            if user_models:
                MODEL_CONFIGS.update(user_models)
                print(f"[Qwen3VL] âœ… å·²åŠ è½½ {len(user_models)} ä¸ªè‡ªå®šä¹‰æ¨¡å‹")
            else:
                print("[Qwen3VL] âš ï¸ æ‰¾åˆ° custom_models.json ä½†æ²¡æœ‰æœ‰æ•ˆçš„æ¨¡å‹æ¡ç›®")
        except Exception as e:
            print(f"[Qwen3VL] âš ï¸ åŠ è½½ custom_models.json å¤±è´¥ â†’ {e}")
    else:
        print("[Qwen3VL] â„¹ï¸ æœªæ‰¾åˆ° custom_models.jsonï¼Œè·³è¿‡è‡ªå®šä¹‰æ¨¡å‹")

if not MODEL_CONFIGS:
    load_model_configs()

class Quantization(str, Enum):
    """é‡åŒ–é€‰é¡¹æšä¸¾"""
    Q4_BIT = "4-bit (èŠ‚çœæ˜¾å­˜)"
    Q8_BIT = "8-bit (å¹³è¡¡)"
    NONE = "None (FP16)"
    
    @classmethod
    def get_values(cls):
        return [item.value for item in cls]

def get_model_info(model_name: str) -> dict:
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    return MODEL_CONFIGS.get(model_name, {})

def get_device_info() -> dict:
    """è·å–è®¾å¤‡ä¿¡æ¯"""
    gpu_info = {}
    if torch.cuda.is_available():
        props = torch.cuda.get_device_properties(0)
        total_mem = props.total_memory / 1024**3
        gpu_info = {
            "available": True,
            "total_memory": total_mem,
            "free_memory": total_mem - (torch.cuda.memory_allocated(0) / 1024**3)
        }
    else:
        gpu_info = {"available": False, "total_memory": 0, "free_memory": 0}

    sys_mem = psutil.virtual_memory()
    sys_mem_info = {
        "total": sys_mem.total / 1024**3,
        "available": sys_mem.available / 1024**3
    }

    device_info = {
        "gpu": gpu_info,
        "system_memory": sys_mem_info,
        "device_type": "cpu",
        "recommended_device": "cpu",
        "memory_sufficient": True,
        "warning_message": ""
    }

    if platform.system() == "Darwin" and platform.processor() == "arm":
        device_info.update({
            "device_type": "apple_silicon",
            "recommended_device": "mps"
        })
        if sys_mem_info["total"] < 16:
            device_info.update({
                "memory_sufficient": False,
                "warning_message": "Apple Silicon å†…å­˜å°äº 16GBï¼Œæ€§èƒ½å¯èƒ½å—å½±å“"
            })
    elif gpu_info["available"]:
        device_info.update({
            "device_type": "nvidia_gpu",
            "recommended_device": "cuda"
        })
        if gpu_info["total_memory"] < 8:
            device_info.update({
                "memory_sufficient": False,
                "warning_message": "GPU æ˜¾å­˜å°äº 8GBï¼Œæ€§èƒ½å¯èƒ½ä¸‹é™"
            })
    
    return device_info

def check_memory_requirements(model_name: str, quantization: str, device_info: dict) -> str:
    """æ£€æŸ¥å†…å­˜éœ€æ±‚å¹¶è‡ªåŠ¨è°ƒæ•´é‡åŒ–çº§åˆ«"""
    model_info = get_model_info(model_name)
    vram_req = model_info.get("vram_requirement", {})
    
    quant_map = {
        Quantization.Q4_BIT: vram_req.get("4bit", 0),
        Quantization.Q8_BIT: vram_req.get("8bit", 0),
        Quantization.NONE: vram_req.get("full", 0)
    }
    
    base_memory = quant_map.get(quantization, 0)
    device = device_info["recommended_device"]
    use_cpu_mps = device in ["cpu", "mps"]
    
    required_mem = base_memory * (1.5 if use_cpu_mps else 1.0)
    available_mem = device_info["system_memory"]["available"] if use_cpu_mps else device_info["gpu"]["free_memory"]
    mem_type = "ç³»ç»Ÿå†…å­˜" if use_cpu_mps else "GPUæ˜¾å­˜"

    if required_mem * 1.2 > available_mem:
        print(f"è­¦å‘Š: {mem_type} ä¸è¶³ ({available_mem:.2f}GB å¯ç”¨)ã€‚é™ä½é‡åŒ–çº§åˆ«...")
        if quantization == Quantization.NONE:
            return Quantization.Q8_BIT
        if quantization == Quantization.Q8_BIT:
            return Quantization.Q4_BIT
        raise RuntimeError(f"{mem_type} ä¸è¶³ï¼Œå³ä½¿ä½¿ç”¨ 4-bit é‡åŒ–ä¹Ÿæ— æ³•è¿è¡Œ")
    
    return quantization

def check_flash_attention() -> bool:
    """æ£€æŸ¥æ˜¯å¦æ”¯æŒ Flash Attention 2"""
    try:
        import flash_attn
        if torch.cuda.is_available():
            major, _ = torch.cuda.get_device_capability()
            return major >= 8
    except ImportError:
        return False
    return False

class ImageProcessor:
    """å›¾åƒå¤„ç†å™¨"""
    def to_pil(self, image_tensor: torch.Tensor) -> Image.Image:
        """å°† ComfyUI å›¾åƒå¼ é‡è½¬æ¢ä¸º PIL Image"""
        if image_tensor.dim() == 4:
            image_tensor = image_tensor[0]
        image_np = (image_tensor.cpu().numpy() * 255).astype(np.uint8)
        return Image.fromarray(image_np)

class ModelDownloader:
    """æ¨¡å‹ä¸‹è½½å™¨
    
    æ¨¡å‹å­˜å‚¨è·¯å¾„ï¼šComfyUI/models/prompt_generator/
    """
    def __init__(self, configs):
        self.configs = configs
        # ä¿®æ”¹æ¨¡å‹å­˜å‚¨è·¯å¾„ä¸º prompt_generator æ–‡ä»¶å¤¹
        self.models_dir = Path(folder_paths.models_dir) / "prompt_generator"
        self.models_dir.mkdir(parents=True, exist_ok=True)

    def ensure_model_available(self, model_name):
        """ç¡®ä¿æ¨¡å‹å¯ç”¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä¸‹è½½
        
        æ¨¡å‹ä¼šç›´æ¥ä¸‹è½½åˆ° ComfyUI/models/prompt_generator/ ç›®å½•
        å¦‚æœæ¨¡å‹å·²å­˜åœ¨ï¼Œåˆ™ç›´æ¥ä½¿ç”¨ï¼Œä¸ä¼šé‡å¤ä¸‹è½½
        æ”¯æŒ HuggingFace å’Œ ModelScope ä¸¤ç§æ¥æº
        """
        model_info = self.configs.get(model_name)
        if not model_info:
            raise ValueError(f"æ¨¡å‹ '{model_name}' æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°")

        repo_id = model_info['repo_id']
        source = model_info.get('source', 'huggingface')  # é»˜è®¤ä½¿ç”¨ HuggingFace
        model_folder_name = repo_id.split('/')[-1]
        model_path = self.models_dir / model_folder_name
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å®Œæ•´ä¸‹è½½ï¼ˆæ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼‰
        config_file = model_path / "config.json"
        model_file = model_path / "model.safetensors"
        # æœ‰äº›æ¨¡å‹ä½¿ç”¨åˆ†ç‰‡å­˜å‚¨
        model_index = model_path / "model.safetensors.index.json"
        
        if model_path.exists() and config_file.exists():
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆå®Œæ•´æ¨¡å‹æˆ–åˆ†ç‰‡æ¨¡å‹ï¼‰
            if model_file.exists() or model_index.exists():
                print(f"âœ… æ¨¡å‹ '{model_name}' å·²å­˜åœ¨äº {model_path}")
                print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {model_path}")
                return str(model_path)
            else:
                print(f"âš ï¸ æ¨¡å‹ç›®å½•å­˜åœ¨ä½†æ–‡ä»¶ä¸å®Œæ•´ï¼Œå°†é‡æ–°ä¸‹è½½...")
        
        # æ£€æŸ¥ ModelScope æ¨¡å‹æ˜¯å¦éœ€è¦å®‰è£…ä¾èµ–
        if source == 'modelscope' and not MODELSCOPE_AVAILABLE:
            raise RuntimeError(
                f"æ¨¡å‹ '{model_name}' æ¥è‡ª ModelScopeï¼Œä½† ModelScope åº“æœªå®‰è£…ã€‚\n"
                f"è¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
                f"pip install modelscope\n"
                f"æˆ–è€…ä½¿ç”¨ HuggingFace é•œåƒç«™æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°: {model_path}"
            )
        
        print(f"ğŸ“¥ æ­£åœ¨ä» {source.upper()} ä¸‹è½½æ¨¡å‹ '{model_name}' åˆ° {model_path}...")
        print(f"ğŸ“ ç›®æ ‡è·¯å¾„: {model_path}")
        print("â³ æç¤ºï¼šé¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        model_path.mkdir(parents=True, exist_ok=True)
        
        # æ ¹æ®æ¥æºé€‰æ‹©ä¸‹è½½å‡½æ•°
        if source == 'modelscope':
            snapshot_download_func = ms_snapshot_download
            download_kwargs = {
                "model_id": repo_id,
                "cache_dir": str(model_path.parent),
                "local_dir": str(model_path),
            }
            source_url = f"https://modelscope.cn/models/{repo_id}"
        else:
            snapshot_download_func = hf_snapshot_download
            download_kwargs = {
                "repo_id": repo_id,
                "local_dir": str(model_path),
                "local_dir_use_symlinks": False,
                "ignore_patterns": ["*.md", "*.txt", ".gitattributes"],
                "resume_download": True,
                "max_workers": 4
            }
            source_url = f"https://huggingface.co/{repo_id}"
        
        # æ·»åŠ é‡è¯•æœºåˆ¶ï¼Œè§£å†³ç½‘ç»œè¿æ¥é—®é¢˜
        max_retries = 3
        for attempt in range(max_retries):
            try:
                downloaded_path = snapshot_download_func(**download_kwargs)
                print(f"âœ… æ¨¡å‹ '{model_name}' ä¸‹è½½å®Œæˆï¼")
                print(f"ğŸ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
                return str(model_path)
            except Exception as e:
                if attempt < max_retries - 1:
                    print(f"âš ï¸ ä¸‹è½½å¤±è´¥ï¼ˆå°è¯• {attempt + 1}/{max_retries}ï¼‰: {str(e)}")
                    print(f"â³ ç­‰å¾… 5 ç§’åé‡è¯•...")
                    time.sleep(5)
                else:
                    print(f"âŒ ä¸‹è½½å¤±è´¥ï¼Œå·²é‡è¯• {max_retries} æ¬¡")
                    error_msg = f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {str(e)}\nå»ºè®®ï¼š\n"
                    if source == 'modelscope':
                        error_msg += (
                            f"1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                            f"2. ç¡®ä¿å·²å®‰è£… ModelScope: pip install modelscope\n"
                            f"3. æ‰‹åŠ¨ä» {source_url} ä¸‹è½½æ¨¡å‹åˆ°: {model_path}\n"
                        )
                    else:
                        error_msg += (
                            f"1. æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦æ­£å¸¸\n"
                            f"2. è®¾ç½® HF_ENDPOINT ç¯å¢ƒå˜é‡ä½¿ç”¨é•œåƒæºï¼ˆå¦‚ï¼šhttps://hf-mirror.comï¼‰\n"
                            f"3. æ‰‹åŠ¨ä» {source_url} ä¸‹è½½æ¨¡å‹åˆ°: {model_path}\n"
                        )
                    raise RuntimeError(error_msg)

class Qwen3VL_Advanced:
    """Qwen3-VL é«˜çº§èŠ‚ç‚¹ - æ”¯æŒå›¾åƒå’Œè§†é¢‘ç†è§£"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.current_model_name = None
        self.current_quantization = None
        self.current_device = None
        self.device_info = get_device_info()
        self.downloader = ModelDownloader(MODEL_CONFIGS)
        self.image_processor = ImageProcessor()
        print(f"Qwen3VL èŠ‚ç‚¹å·²åˆå§‹åŒ–ã€‚è®¾å¤‡: {self.device_info['device_type']}")
        if not self.device_info["memory_sufficient"]:
            print(f"è­¦å‘Š: {self.device_info['warning_message']}")

    def clear_model_resources(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if self.model is not None:
            print("é‡Šæ”¾æ¨¡å‹èµ„æº...")
            del self.model, self.processor, self.tokenizer
            self.model = self.processor = self.tokenizer = None
            self.current_model_name = self.current_quantization = self.current_device = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def load_model(self, model_name: str, quantization_str: str, device: str = "auto"):
        """åŠ è½½æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            quantization_str: é‡åŒ–çº§åˆ«å­—ç¬¦ä¸²
            device: è®¾å¤‡ç±»å‹ (auto/cuda/cpu/mps)
        
        Raises:
            ValueError: å½“ GPU ä¸æ”¯æŒ FP8 æ¨¡å‹æˆ–ä½¿ç”¨ abliterated æ¨¡å‹æ—¶
        """
        effective_device = self.device_info["recommended_device"] if device == "auto" else device
        
        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”é…ç½®ç›¸åŒï¼Œåˆ™è·³è¿‡
        if (self.model is not None and 
            self.current_model_name == model_name and 
            self.current_quantization == quantization_str and 
            self.current_device == effective_device):
            return

        self.clear_model_resources()

        model_info = get_model_info(model_name)
        
        # æ£€æŸ¥ abliterated æ¨¡å‹çš„è­¦å‘Š
        if model_info.get("abliterated"):
            warning_msg = model_info.get("warning", "æ­¤æ¨¡å‹å·²ç§»é™¤å®‰å…¨è¿‡æ»¤")
            print(f"\nâš ï¸  è­¦å‘Š: {warning_msg}\n")
        
        # æ£€æŸ¥ FP8 é‡åŒ–æ¨¡å‹çš„ GPU è®¡ç®—èƒ½åŠ›è¦æ±‚
        if model_info.get("quantized"):
            if self.device_info["gpu"]["available"]:
                major, minor = torch.cuda.get_device_capability()
                cc = major + minor / 10
                if cc < 8.9:
                    raise ValueError(
                        f"FP8 æ¨¡å‹éœ€è¦è®¡ç®—èƒ½åŠ› 8.9 æˆ–æ›´é«˜çš„ GPU (ä¾‹å¦‚ RTX 4090)ã€‚"
                        f"æ‚¨çš„ GPU è®¡ç®—èƒ½åŠ›ä¸º {cc}ã€‚è¯·é€‰æ‹©é FP8 æ¨¡å‹ã€‚"
                    )

        model_path = self.downloader.ensure_model_available(model_name)
        adjusted_quantization = check_memory_requirements(model_name, quantization_str, self.device_info)
        
        quant_config, load_dtype = None, torch.float16
        
        # ä»…å¯¹éé¢„é‡åŒ–æ¨¡å‹åº”ç”¨é‡åŒ–é…ç½®
        if not get_model_info(model_name).get("quantized", False):
            if adjusted_quantization == Quantization.Q4_BIT:
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
                load_dtype = None
            elif adjusted_quantization == Quantization.Q8_BIT:
                quant_config = BitsAndBytesConfig(load_in_8bit=True)
                load_dtype = None

        device_map = "auto"
        if effective_device == "cuda" and torch.cuda.is_available():
            device_map = {"": 0}

        # æ„å»ºæ¨¡å‹åŠ è½½å‚æ•°
        load_kwargs = {
            "device_map": device_map,
            "torch_dtype": load_dtype,
            "attn_implementation": "flash_attention_2" if check_flash_attention() else "sdpa",
            "use_safetensors": True,
            "trust_remote_code": True  # abliterated æ¨¡å‹éœ€è¦
        }
        
        if quant_config:
            load_kwargs["quantization_config"] = quant_config

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ '{model_name}'...")
        # åŠ è½½æ¨¡å‹ã€å¤„ç†å™¨å’Œåˆ†è¯å™¨
        self.model = AutoModelForImageTextToText.from_pretrained(model_path, **load_kwargs).eval()
        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        self.current_model_name = model_name
        self.current_quantization = quantization_str
        self.current_device = effective_device
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")

    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        model_names = [name for name in MODEL_CONFIGS.keys() if not name.startswith('_')]
        default_model = model_names[4] if len(model_names) > 4 else model_names[0]
        preset_prompts = MODEL_CONFIGS.get("_preset_prompts", ["è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡"])

        return {
            "required": {
                "ğŸ¤– æ¨¡å‹é€‰æ‹©": (model_names, {"default": default_model}),
                "âš™ï¸ é‡åŒ–çº§åˆ«": (list(Quantization.get_values()), {"default": Quantization.NONE}),
                "ğŸ’­ é¢„è®¾æç¤ºè¯": (preset_prompts, {"default": preset_prompts[2]}),
                "âœï¸ è‡ªå®šä¹‰æç¤ºè¯": ("STRING", {
                    "default": "",
                    "multiline": True,
                    "placeholder": "å¯é€‰æ‹©é¢„è®¾æç¤ºè¯æˆ–è¾“å…¥è‡ªå®šä¹‰æç¤ºè¯"
                }),
                "ğŸ”¢ æœ€å¤§ä»¤ç‰Œæ•°": ("INT", {"default": 1024, "min": 64, "max": 4096, "step": 16}),
                "ğŸŒ¡ï¸ é‡‡æ ·æ¸©åº¦": ("FLOAT", {"default": 0.6, "min": 0.1, "max": 1.0, "step": 0.1}),
                "ğŸ¯ æ ¸é‡‡æ ·å‚æ•°": ("FLOAT", {"default": 0.9, "min": 0.0, "max": 1.0, "step": 0.01}),
                "ğŸ” æŸæœç´¢æ•°é‡": ("INT", {"default": 1, "min": 1, "max": 10, "step": 1}),
                "ğŸš« é‡å¤æƒ©ç½š": ("FLOAT", {"default": 1.2, "min": 0.0, "max": 2.0, "step": 0.01}),
                "ğŸ¬ è§†é¢‘å¸§æ•°": ("INT", {"default": 16, "min": 1, "max": 64, "step": 1}),
                "ğŸ’» è®¾å¤‡é€‰æ‹©": (["auto", "cuda", "cpu", "mps"], {"default": "auto"}),
                "ğŸ”„ ä¿æŒæ¨¡å‹åŠ è½½": ("BOOLEAN", {"default": False}),
                "ğŸ² éšæœºç§å­": ("INT", {"default": 1, "min": 1, "max": 0xFFFFFFFFFFFFFFFF}),
                "ğŸ® ç§å­æ§åˆ¶": (["éšæœº", "å›ºå®š"], {"default": "éšæœº"}),
            },
            "optional": {
                "ğŸ–¼ï¸ å›¾åƒ1": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ2": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ3": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ4": ("IMAGE",),
                "ğŸ¥ è§†é¢‘": ("IMAGE",),
                "ğŸ¯ Qwen3VLé¢å¤–é€‰é¡¹": ("QWEN3VL_EXTRA_OPTIONS", {
                    "tooltip": "å¯é€‰çš„Qwen3VLé¢å¤–é€‰é¡¹ï¼Œè¿æ¥Qwen3VLé¢å¤–é€‰é¡¹èŠ‚ç‚¹"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("æ–‡æœ¬è¾“å‡º",)
    FUNCTION = "process"
    CATEGORY = "ğŸ­å¤§ç‚®-Qwen3VL"

    @torch.no_grad()
    def process(self, **kwargs):
        """å¤„ç†å›¾åƒæˆ–è§†é¢‘è¾“å…¥å¹¶ç”Ÿæˆæ–‡æœ¬ - ä½¿ç”¨kwargså¤„ç†å¸¦emojiçš„å‚æ•°å"""
        # æå–å‚æ•°ï¼ˆå…¼å®¹å¸¦emojiçš„å‚æ•°åï¼‰
        æ¨¡å‹åç§° = kwargs.get("ğŸ¤– æ¨¡å‹é€‰æ‹©")
        é‡åŒ–çº§åˆ« = kwargs.get("âš™ï¸ é‡åŒ–çº§åˆ«")
        é¢„è®¾æç¤ºè¯ = kwargs.get("ğŸ’­ é¢„è®¾æç¤ºè¯")
        æœ€å¤§ä»¤ç‰Œæ•° = kwargs.get("ğŸ”¢ æœ€å¤§ä»¤ç‰Œæ•°")
        é‡‡æ ·æ¸©åº¦ = kwargs.get("ğŸŒ¡ï¸ é‡‡æ ·æ¸©åº¦")
        æ ¸é‡‡æ ·å‚æ•° = kwargs.get("ğŸ¯ æ ¸é‡‡æ ·å‚æ•°")
        é‡å¤æƒ©ç½š = kwargs.get("ğŸš« é‡å¤æƒ©ç½š")
        æŸæœç´¢æ•°é‡ = kwargs.get("ğŸ” æŸæœç´¢æ•°é‡")
        è§†é¢‘å¸§æ•° = kwargs.get("ğŸ¬ è§†é¢‘å¸§æ•°")
        è®¾å¤‡é€‰æ‹© = kwargs.get("ğŸ’» è®¾å¤‡é€‰æ‹©")
        éšæœºç§å­ = kwargs.get("ğŸ² éšæœºç§å­")
        è‡ªå®šä¹‰æç¤ºè¯ = kwargs.get("âœï¸ è‡ªå®šä¹‰æç¤ºè¯", "")
        å›¾åƒ1 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ1")
        å›¾åƒ2 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ2")
        å›¾åƒ3 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ3")
        å›¾åƒ4 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ4")
        è§†é¢‘ = kwargs.get("ğŸ¥ è§†é¢‘")
        ä¿æŒæ¨¡å‹åŠ è½½ = kwargs.get("ğŸ”„ ä¿æŒæ¨¡å‹åŠ è½½", True)
        ç§å­æ§åˆ¶ = kwargs.get("ğŸ® ç§å­æ§åˆ¶", "éšæœº")
        extra_options = kwargs.get("ğŸ¯ Qwen3VLé¢å¤–é€‰é¡¹", None)
        start_time = time.time()
        
        # æ ¹æ®ç§å­æ§åˆ¶è®¾ç½®éšæœºç§å­
        if ç§å­æ§åˆ¶ == "å›ºå®š":
            torch.manual_seed(éšæœºç§å­)
        else:
            torch.manual_seed(int(time.time()))
        
        try:
            self.load_model(æ¨¡å‹åç§°, é‡åŒ–çº§åˆ«, è®¾å¤‡é€‰æ‹©)
            effective_device = self.current_device
            
            # ç¡®å®šä½¿ç”¨çš„æç¤ºè¯ï¼ˆå›¾åƒ/è§†é¢‘åæ¨ä¸“ç”¨ï¼‰
            prompt_text = SYSTEM_PROMPTS.get(é¢„è®¾æç¤ºè¯, é¢„è®¾æç¤ºè¯)
            if è‡ªå®šä¹‰æç¤ºè¯ and è‡ªå®šä¹‰æç¤ºè¯.strip():
                prompt_text = è‡ªå®šä¹‰æç¤ºè¯.strip()
            
            # åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹ç”Ÿæˆå¢å¼ºæç¤ºè¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if extra_options:
                try:
                    import qwen3vl_extra_options
                    prompt_text = qwen3vl_extra_options.Qwen3VL_ExtraOptions.build_enhanced_prompt(prompt_text, extra_options)
                    print(f"âœ… å·²åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹å¢å¼ºæç¤ºè¯")
                except (ImportError, AttributeError) as e:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥Qwen3VLé¢å¤–é€‰é¡¹æ¨¡å— ({e})ï¼Œä½¿ç”¨åŸºç¡€æç¤ºè¯")
            
            # æ„å»ºå¯¹è¯æ¶ˆæ¯
            conversation = [{"role": "user", "content": []}]
            
            # æ·»åŠ å¤šä¸ªå›¾åƒ
            for i, image in enumerate([å›¾åƒ1, å›¾åƒ2, å›¾åƒ3, å›¾åƒ4], 1):
                if image is not None:
                    conversation[0]["content"].append({
                        "type": "image",
                        "image": self.image_processor.to_pil(image)
                    })
            
            # æ·»åŠ è§†é¢‘ï¼ˆä½œä¸ºå¤šå¸§å›¾åƒåºåˆ—ï¼‰
            if è§†é¢‘ is not None:
                video_frames = [
                    Image.fromarray((frame.cpu().numpy() * 255).astype(np.uint8))
                    for frame in è§†é¢‘
                ]
                
                # é‡‡æ ·è§†é¢‘å¸§
                if len(video_frames) > è§†é¢‘å¸§æ•°:
                    indices = np.linspace(0, len(video_frames) - 1, è§†é¢‘å¸§æ•°, dtype=int)
                    sampled_frames = [video_frames[i] for i in indices]
                else:
                    sampled_frames = video_frames

                # ç¡®ä¿è‡³å°‘æœ‰2å¸§ï¼ˆQwen3-VL è¦æ±‚ï¼‰
                if sampled_frames and len(sampled_frames) == 1:
                    sampled_frames.append(sampled_frames[0])
                    
                if sampled_frames:
                    conversation[0]["content"].append({
                        "type": "video",
                        "video": sampled_frames
                    })

            # æ·»åŠ æ–‡æœ¬æç¤º
            conversation[0]["content"].append({
                "type": "text",
                "text": prompt_text
            })

            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # æå–å›¾åƒå’Œè§†é¢‘ç”¨äºå¤„ç†å™¨
            pil_images = [
                item['image'] for item in conversation[0]['content']
                if item['type'] == 'image'
            ]
            video_frames_list = [
                frame for item in conversation[0]['content']
                if item['type'] == 'video'
                for frame in item['video']
            ]
            videos_arg = [video_frames_list] if video_frames_list else None
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt,
                images=pil_images if pil_images else None,
                videos=videos_arg,
                return_tensors="pt"
            )
            
            # å°†è¾“å…¥ç§»åˆ°è®¾å¤‡
            model_inputs = {
                k: v.to(effective_device)
                for k, v in inputs.items()
                if torch.is_tensor(v)
            }

            # è®¾ç½®åœæ­¢æ ‡è®°
            stop_tokens = [self.tokenizer.eos_token_id]
            if hasattr(self.tokenizer, 'eot_id'):
                stop_tokens.append(self.tokenizer.eot_id)

            # ç”Ÿæˆå‚æ•°
            gen_kwargs = {
                "max_new_tokens": æœ€å¤§ä»¤ç‰Œæ•°,
                "repetition_penalty": é‡å¤æƒ©ç½š,
                "num_beams": æŸæœç´¢æ•°é‡,
                "eos_token_id": stop_tokens,
                "pad_token_id": self.tokenizer.pad_token_id
            }
            
            if æŸæœç´¢æ•°é‡ > 1:
                gen_kwargs["do_sample"] = False
            else:
                gen_kwargs.update({
                    "do_sample": True,
                    "temperature": é‡‡æ ·æ¸©åº¦,
                    "top_p": æ ¸é‡‡æ ·å‚æ•°
                })

            # ç”Ÿæˆæ–‡æœ¬
            outputs = self.model.generate(**model_inputs, **gen_kwargs)
            input_ids_len = model_inputs["input_ids"].shape[1]
            text = self.tokenizer.decode(
                outputs[0, input_ids_len:],
                skip_special_tokens=True
            )
            
            print(f"ç”Ÿæˆå®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f} ç§’")
            return (text.strip(),)

        except (ValueError, RuntimeError) as e:
            error_message = f"é”™è¯¯: {str(e)}"
            print(error_message)
            return (error_message,)
        finally:
            if not ä¿æŒæ¨¡å‹åŠ è½½:
                self.clear_model_resources()


class Qwen3VL_Chat:
    """Qwen3-VL æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹ - æ”¯æŒå¤šæ¨¡æ€LLMå¯¹è¯"""
    
    def __init__(self):
        self.model = None
        self.processor = None
        self.tokenizer = None
        self.current_model_name = None
        self.current_quantization = None
        self.current_device = None
        self.device_info = get_device_info()
        self.downloader = ModelDownloader(MODEL_CONFIGS)
        self.image_processor = ImageProcessor()
        print(f"Qwen3VL æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹å·²åˆå§‹åŒ–ã€‚è®¾å¤‡: {self.device_info['device_type']}")
        if not self.device_info["memory_sufficient"]:
            print(f"è­¦å‘Š: {self.device_info['warning_message']}")

    def clear_model_resources(self):
        """æ¸…ç†æ¨¡å‹èµ„æº"""
        if self.model is not None:
            print("é‡Šæ”¾æ¨¡å‹èµ„æº...")
            del self.model, self.processor, self.tokenizer
            self.model = self.processor = self.tokenizer = None
            self.current_model_name = self.current_quantization = self.current_device = None
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

    def load_model(self, model_name: str, quantization_str: str, device: str = "auto"):
        """åŠ è½½æ¨¡å‹"""
        effective_device = self.device_info["recommended_device"] if device == "auto" else device
        
        # å¦‚æœæ¨¡å‹å·²åŠ è½½ä¸”é…ç½®ç›¸åŒï¼Œåˆ™è·³è¿‡
        if (self.model is not None and 
            self.current_model_name == model_name and 
            self.current_quantization == quantization_str and 
            self.current_device == effective_device):
            return

        self.clear_model_resources()

        model_info = get_model_info(model_name)
        
        # æ£€æŸ¥ abliterated æ¨¡å‹çš„è­¦å‘Š
        if model_info.get("abliterated"):
            warning_msg = model_info.get("warning", "æ­¤æ¨¡å‹å·²ç§»é™¤å®‰å…¨è¿‡æ»¤")
            print(f"\nâš ï¸  è­¦å‘Š: {warning_msg}\n")
        
        # æ£€æŸ¥ FP8 é‡åŒ–æ¨¡å‹çš„ GPU è®¡ç®—èƒ½åŠ›è¦æ±‚
        if model_info.get("quantized"):
            if self.device_info["gpu"]["available"]:
                major, minor = torch.cuda.get_device_capability()
                cc = major + minor / 10
                if cc < 8.9:
                    raise ValueError(
                        f"FP8 æ¨¡å‹éœ€è¦è®¡ç®—èƒ½åŠ› 8.9 æˆ–æ›´é«˜çš„ GPU (ä¾‹å¦‚ RTX 4090)ã€‚"
                        f"æ‚¨çš„ GPU è®¡ç®—èƒ½åŠ›ä¸º {cc}ã€‚è¯·é€‰æ‹©é FP8 æ¨¡å‹ã€‚"
                    )

        model_path = self.downloader.ensure_model_available(model_name)
        adjusted_quantization = check_memory_requirements(model_name, quantization_str, self.device_info)
        
        quant_config, load_dtype = None, torch.float16
        
        # ä»…å¯¹éé¢„é‡åŒ–æ¨¡å‹åº”ç”¨é‡åŒ–é…ç½®
        if not get_model_info(model_name).get("quantized", False):
            if adjusted_quantization == Quantization.Q4_BIT:
                quant_config = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16,
                    bnb_4bit_quant_type="nf4",
                    bnb_4bit_use_double_quant=True
                )
                load_dtype = None
            elif adjusted_quantization == Quantization.Q8_BIT:
                quant_config = BitsAndBytesConfig(load_in_8bit=True)
                load_dtype = None

        device_map = "auto"
        if effective_device == "cuda" and torch.cuda.is_available():
            device_map = {"": 0}

        # æ„å»ºæ¨¡å‹åŠ è½½å‚æ•°
        load_kwargs = {
            "device_map": device_map,
            "torch_dtype": load_dtype,
            "attn_implementation": "flash_attention_2" if check_flash_attention() else "sdpa",
            "use_safetensors": True,
            "trust_remote_code": True
        }
        
        if quant_config:
            load_kwargs["quantization_config"] = quant_config

        print(f"æ­£åœ¨åŠ è½½æ¨¡å‹ '{model_name}'...")
        # åŠ è½½æ¨¡å‹ã€å¤„ç†å™¨å’Œåˆ†è¯å™¨
        self.model = AutoModelForImageTextToText.from_pretrained(model_path, **load_kwargs).eval()
        self.processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        
        self.current_model_name = model_name
        self.current_quantization = quantization_str
        self.current_device = effective_device
        print("æ¨¡å‹åŠ è½½æˆåŠŸ")

    @classmethod
    def INPUT_TYPES(cls):
        """å®šä¹‰æ™ºèƒ½å¯¹è¯èŠ‚ç‚¹è¾“å…¥ç±»å‹"""
        model_names = [name for name in MODEL_CONFIGS.keys() if not name.startswith('_')]
        default_model = model_names[4] if len(model_names) > 4 else model_names[0]

        return {
            "required": {
                "ğŸ¤– æ¨¡å‹é€‰æ‹©": (model_names, {"default": default_model}),
                "âš™ï¸ é‡åŒ–çº§åˆ«": (list(Quantization.get_values()), {"default": Quantization.NONE}),
                "ğŸ’¬ ç”¨æˆ·è¾“å…¥": ("STRING", {
                    "default": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
                    "multiline": True,
                    "placeholder": "è¾“å…¥ä½ æƒ³è¦å¯¹è¯çš„å†…å®¹"
                }),
                "ğŸ­ ç³»ç»Ÿè§’è‰²å®šä¹‰": ("STRING", {
                    "default": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šã€å‹å¥½ä¸”ä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚",
                    "multiline": True,
                    "placeholder": "å®šä¹‰AIçš„è§’è‰²å’Œè¡Œä¸ºæ–¹å¼"
                }),
                "ğŸŒ¡ï¸ æ¸©åº¦": ("FLOAT", {"default": 0.7, "min": 0.1, "max": 1.0, "step": 0.1}),
                "ğŸ¯ Top-P": ("FLOAT", {"default": 0.90, "min": 0.0, "max": 1.0, "step": 0.01}),
                "ğŸ“ æœ€å¤§é•¿åº¦": ("INT", {"default": 2048, "min": 64, "max": 4096, "step": 16}),
                "ğŸ² éšæœºç§å­": ("INT", {"default": 0, "min": 0, "max": 0xFFFFFFFFFFFFFFFF}),
                "ğŸ® ç§å­æ§åˆ¶": (["éšæœº", "å›ºå®š"], {"default": "éšæœº"}),
                "ğŸ”„ ä¿æŒæ¨¡å‹åŠ è½½": ("BOOLEAN", {"default": False}),
            },
            "optional": {
                "ğŸ–¼ï¸ å›¾åƒ1": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ2": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ3": ("IMAGE",),
                "ğŸ–¼ï¸ å›¾åƒ4": ("IMAGE",),
                "ğŸ¯ Qwen3VLé¢å¤–é€‰é¡¹": ("QWEN3VL_EXTRA_OPTIONS", {
                    "tooltip": "å¯é€‰çš„Qwen3VLé¢å¤–é€‰é¡¹ï¼Œè¿æ¥Qwen3VLé¢å¤–é€‰é¡¹èŠ‚ç‚¹"
                }),
            }
        }

    RETURN_TYPES = ("STRING",)
    RETURN_NAMES = ("AIå›å¤",)
    FUNCTION = "chat"
    CATEGORY = "ğŸ­å¤§ç‚®-Qwen3VL"

    @torch.no_grad()
    def chat(self, **kwargs):
        """æ™ºèƒ½å¯¹è¯å¤„ç†å‡½æ•°"""
        # æå–å‚æ•°ï¼ˆå…¼å®¹å¸¦emojiçš„å‚æ•°åï¼‰
        æ¨¡å‹åç§° = kwargs.get("ğŸ¤– æ¨¡å‹é€‰æ‹©")
        é‡åŒ–çº§åˆ« = kwargs.get("âš™ï¸ é‡åŒ–çº§åˆ«")
        ç”¨æˆ·è¾“å…¥ = kwargs.get("ğŸ’¬ ç”¨æˆ·è¾“å…¥")
        ç³»ç»Ÿè§’è‰²å®šä¹‰ = kwargs.get("ğŸ­ ç³»ç»Ÿè§’è‰²å®šä¹‰")
        æ¸©åº¦ = kwargs.get("ğŸŒ¡ï¸ æ¸©åº¦")
        æœ€å¤§é•¿åº¦ = kwargs.get("ğŸ“ æœ€å¤§é•¿åº¦")
        éšæœºç§å­ = kwargs.get("ğŸ² éšæœºç§å­")
        ç§å­æ§åˆ¶ = kwargs.get("ğŸ® ç§å­æ§åˆ¶")
        ä¿æŒæ¨¡å‹åŠ è½½ = kwargs.get("ğŸ”„ ä¿æŒæ¨¡å‹åŠ è½½")
        å›¾åƒ1 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ1")
        å›¾åƒ2 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ2")
        å›¾åƒ3 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ3")
        å›¾åƒ4 = kwargs.get("ğŸ–¼ï¸ å›¾åƒ4")
        extra_options = kwargs.get("ğŸ¯ Qwen3VLé¢å¤–é€‰é¡¹", None)
        # å¤„ç†Top-På‚æ•°ï¼ˆå…¼å®¹æ–°æ—§ç‰ˆæœ¬ï¼‰
        top_p = kwargs.get("ğŸ¯ Top-P", 0.90)
        
        start_time = time.time()
        
        # æ ¹æ®ç§å­æ§åˆ¶è®¾ç½®éšæœºç§å­
        if ç§å­æ§åˆ¶ == "å›ºå®š":
            torch.manual_seed(éšæœºç§å­)
        else:
            torch.manual_seed(int(time.time()))
        
        try:
            self.load_model(æ¨¡å‹åç§°, é‡åŒ–çº§åˆ«, "auto")
            effective_device = self.current_device
            
            # å¤„ç†ç³»ç»Ÿè§’è‰²å®šä¹‰ï¼Œåº”ç”¨é¢å¤–é€‰é¡¹
            system_prompt = ç³»ç»Ÿè§’è‰²å®šä¹‰.strip() if ç³»ç»Ÿè§’è‰²å®šä¹‰ else ""
            
            # åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹å¢å¼ºç³»ç»Ÿæç¤ºè¯ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if extra_options and system_prompt:
                try:
                    import qwen3vl_extra_options
                    system_prompt = qwen3vl_extra_options.Qwen3VL_ExtraOptions.build_enhanced_prompt(system_prompt, extra_options)
                    print(f"âœ… å·²åº”ç”¨Qwen3VLé¢å¤–é€‰é¡¹å¢å¼ºç³»ç»Ÿè§’è‰²")
                except (ImportError, AttributeError) as e:
                    print(f"âš ï¸ è­¦å‘Š: æ— æ³•å¯¼å…¥Qwen3VLé¢å¤–é€‰é¡¹æ¨¡å— ({e})ï¼Œä½¿ç”¨åŸºç¡€ç³»ç»Ÿè§’è‰²")
            
            # æ„å»ºå¯¹è¯æ¶ˆæ¯ï¼Œå…ˆæ·»åŠ ç³»ç»Ÿè§’è‰²å®šä¹‰
            conversation = []
            
            # æ·»åŠ ç³»ç»Ÿè§’è‰²å®šä¹‰ï¼ˆå¦‚æœæä¾›ï¼‰
            if system_prompt:
                conversation.append({
                    "role": "system",
                    "content": [{"type": "text", "text": system_prompt}]
                })
            
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            user_content = []
            
            # æ·»åŠ å¤šä¸ªå›¾åƒ
            for i, image in enumerate([å›¾åƒ1, å›¾åƒ2, å›¾åƒ3, å›¾åƒ4], 1):
                if image is not None:
                    user_content.append({
                        "type": "image",
                        "image": self.image_processor.to_pil(image)
                    })
            
            # æ·»åŠ ç”¨æˆ·æ–‡æœ¬è¾“å…¥
            user_content.append({
                "type": "text",
                "text": ç”¨æˆ·è¾“å…¥
            })
            
            conversation.append({
                "role": "user",
                "content": user_content
            })

            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text_prompt = self.processor.apply_chat_template(
                conversation,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # æå–å›¾åƒç”¨äºå¤„ç†å™¨
            pil_images = []
            for msg in conversation:
                if msg['role'] == 'user':
                    pil_images.extend([
                        item['image'] for item in msg['content']
                        if item['type'] == 'image'
                    ])
            
            # å¤„ç†è¾“å…¥
            inputs = self.processor(
                text=text_prompt,
                images=pil_images if pil_images else None,
                return_tensors="pt"
            )
            
            # å°†è¾“å…¥ç§»åˆ°è®¾å¤‡
            model_inputs = {
                k: v.to(effective_device)
                for k, v in inputs.items()
                if torch.is_tensor(v)
            }

            # è®¾ç½®åœæ­¢æ ‡è®°
            stop_tokens = [self.tokenizer.eos_token_id]
            if hasattr(self.tokenizer, 'eot_id'):
                stop_tokens.append(self.tokenizer.eot_id)

            # æ£€æŸ¥æ˜¯å¦æœ‰æœªè¯†åˆ«çš„å‚æ•°
            remaining_kwargs = {k: v for k, v in kwargs.items() if not k.startswith(('ğŸ¤–', 'âš™ï¸', 'ğŸ’¬', 'ğŸ­', 'ğŸŒ¡ï¸', 'ğŸ¯', 'ğŸ“', 'ğŸ²', 'ğŸ®', 'ğŸ”„', 'ğŸ–¼ï¸'))}
            if remaining_kwargs:
                print(f"[Qwen3VL_Chat] æœªè¯†åˆ«çš„å‚æ•°å·²å¿½ç•¥: {', '.join(remaining_kwargs.keys())}")

            # ç”Ÿæˆå‚æ•°
            gen_kwargs = {
                "max_new_tokens": æœ€å¤§é•¿åº¦,
                "do_sample": True,
                "temperature": æ¸©åº¦,
                "top_p": top_p,
                "eos_token_id": stop_tokens,
                "pad_token_id": self.tokenizer.pad_token_id
            }

            # ç”Ÿæˆæ–‡æœ¬
            outputs = self.model.generate(**model_inputs, **gen_kwargs)
            input_ids_len = model_inputs["input_ids"].shape[1]
            text = self.tokenizer.decode(
                outputs[0, input_ids_len:],
                skip_special_tokens=True
            )
            
            print(f"å¯¹è¯å®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f} ç§’")
            return (text.strip(),)

        except (ValueError, RuntimeError) as e:
            error_message = f"é”™è¯¯: {str(e)}"
            print(error_message)
            return (error_message,)
        finally:
            if not ä¿æŒæ¨¡å‹åŠ è½½:
                self.clear_model_resources()


# èŠ‚ç‚¹æ³¨å†Œ
NODE_CLASS_MAPPINGS = {
    "Qwen3VL_Advanced": Qwen3VL_Advanced,
    "Qwen3VL_Chat": Qwen3VL_Chat,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "Qwen3VL_Advanced": "ğŸ­å¤§ç‚®-Qwen3VL@ç‚®è€å¸ˆçš„å°è¯¾å ‚",
    "Qwen3VL_Chat": "ğŸ­å¤§ç‚®-Qwen3VLæ™ºèƒ½å¯¹è¯@ç‚®è€å¸ˆçš„å°è¯¾å ‚",
}
